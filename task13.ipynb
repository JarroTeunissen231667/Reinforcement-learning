{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "from sim_class import Simulation\n",
    "\n",
    "class OT2Env(gym.Env):\n",
    "    def __init__(self, render=True, max_steps=1000):\n",
    "        super(OT2Env, self).__init__()\n",
    "        self.render_enabled = render  # Use render_enabled to track rendering\n",
    "        self.max_steps = max_steps\n",
    "\n",
    "        # Create the simulation environment\n",
    "        self.sim = Simulation(num_agents=1, render=self.render_enabled)\n",
    "\n",
    "        # Define action and observation spaces\n",
    "        self.action_space = spaces.Box(low=np.array([-1, -1, -1, 0]), high=np.array([1, 1, 1, 1]), shape=(4,), dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(6,), dtype=np.float32)\n",
    "        # keep track of the number of steps\n",
    "        self.steps = 0\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        # being able to set a seed is required for reproducibility\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        # Reset the state of the environment to an initial state\n",
    "        # set a random goal position for the agent, consisting of x, y, and z coordinates within the working area (you determined these values in the previous datalab task)\n",
    "        # ['-0.18700', '0.21950', '0.16950']\n",
    "        # ['0.25300', '-0.17050', '0.16940']\n",
    "        # ['0.25300', '-0.17050', '0.28950']\n",
    "        # ['-0.18700', '0.21950', '0.28950']\n",
    "        # ['-0.18700', '-0.17050', '0.16950']\n",
    "        # ['0.25300', '0.21950', '0.16950']\n",
    "        # ['0.25300', '0.21950', '0.28950']\n",
    "        # ['-0.18700', '-0.17050', '0.28950']\n",
    "\n",
    "        self.goal_position = np.array([\n",
    "            np.random.uniform(-0.18700, 0.25300),\n",
    "            np.random.uniform(-0.17050, 0.21950),\n",
    "            np.random.uniform(0.16940, 0.28950)\n",
    "        ], dtype=np.float32)\n",
    "        # Call the environment reset function\n",
    "        observation = self.sim.reset(num_agents=1)\n",
    "        #print(observation)\n",
    "        # now we need to process the observation and extract the relevant information, the pipette position, convert it to a numpy array, and append the goal position and make sure the array is of type np.float32    \n",
    "        #{'robotId_2': {'joint_states': {'joint_0': {'position': 0.0, 'velocity': 0.0, 'reaction_forces': (0.0, 0.0, 0.0, 0.0, 0.0, 0.0), 'motor_torque': 0.0}, 'joint_1': {'position': 0.0, 'velocity': 0.0, 'reaction_forces': (0.0, 0.0, 0.0, 0.0, 0.0, 0.0), 'motor_torque': 0.0}, 'joint_2': {'position': 0.0, 'velocity': 0.0, 'reaction_forces': (0.0, 0.0, 0.0, 0.0, 0.0, 0.0), 'motor_torque': 0.0}}, 'robot_position': [0.0, 0.0, 0.03], 'pipette_position': [0.073, 0.0895, 0.1195]}}\n",
    "        pipette_position = np.array(observation[f'robotId_{self.sim.robotIds[0]}']['pipette_position'], dtype=np.float32)\n",
    "\n",
    "        observation = np.concatenate([pipette_position, self.goal_position], axis=0)\n",
    "\n",
    "        # Reset the number of steps\n",
    "        self.steps = 0\n",
    "\n",
    "        return observation, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        # Execute one time step within the environment\n",
    "        # since we are only controlling the pipette position, we accept 3 values for the action and need to append 0 for the drop action\n",
    "        action = np.append(action, 0)\n",
    "\n",
    "        # Call the environment step function\n",
    "        observation = self.sim.run([action]) # Why do we need to pass the action as a list? Think about the simulation class.\n",
    "        # now we need to process the observation and extract the relevant information, the pipette position, convert it to a numpy array, and append the goal position and make sure the array is of type np.float32\n",
    "        pipette_position = np.array(observation[f'robotId_{self.sim.robotIds[0]}']['pipette_position'], dtype=np.float32)\n",
    "\n",
    "\n",
    "\n",
    "        observation = np.concatenate([pipette_position, self.goal_position], axis=0)\n",
    "        # Calculate the reward, this is something that you will need to experiment with to get the best results\n",
    "        reward = float(-np.linalg.norm(pipette_position - self.goal_position)) # we can use the L2 norm to calculate the distance between the pipette position and the goal position\n",
    "        \n",
    "        # next we need to check if the if the task has been completed and if the episode should be terminated\n",
    "        # To do this we need to calculate the distance between the pipette position and the goal position and if it is below a certain threshold, we will consider the task complete. \n",
    "        # What is a reasonable threshold? Think about the size of the pipette tip and the size of the plants.\n",
    "        distance = np.linalg.norm(pipette_position - self.goal_position)\n",
    "        if distance < 0.05:\n",
    "            terminated = True\n",
    "            # we can also give the agent a positive reward for completing the task\n",
    "            reward = float(100)\n",
    "        else:\n",
    "            terminated = False\n",
    "\n",
    "        # next we need to check if the episode should be truncated, we can check if the current number of steps is greater than the maximum number of steps\n",
    "        if self.steps >= self.max_steps:\n",
    "            truncated = True\n",
    "        else:\n",
    "            truncated = False\n",
    "\n",
    "        info = {} # we don't need to return any additional information\n",
    "\n",
    "        # increment the number of steps\n",
    "        self.steps += 1\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def render(self):\n",
    "        pass\n",
    "    \n",
    "    def close(self):\n",
    "        self.sim.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goal Position: [0.21703888 0.04528046 0.18339357]\n",
      "Step 1:\n",
      "  Current Position: [0.0735 0.0889 0.1205]\n",
      "  Goal Position: [0.21703888 0.04528046 0.18339357]\n",
      "  Distance to Goal: 162.67 mm\n",
      "Step 2:\n",
      "  Current Position: [0.0744 0.0878 0.1224]\n",
      "  Goal Position: [0.21703888 0.04528046 0.18339357]\n",
      "  Distance to Goal: 160.85 mm\n",
      "Step 3:\n",
      "  Current Position: [0.0757 0.0862 0.1253]\n",
      "  Goal Position: [0.21703888 0.04528046 0.18339357]\n",
      "  Distance to Goal: 158.20 mm\n",
      "Step 4:\n",
      "  Current Position: [0.0776 0.0839 0.1285]\n",
      "  Goal Position: [0.21703888 0.04528046 0.18339357]\n",
      "  Distance to Goal: 154.75 mm\n",
      "Step 5:\n",
      "  Current Position: [0.0799 0.0811 0.1315]\n",
      "  Goal Position: [0.21703888 0.04528046 0.18339357]\n",
      "  Distance to Goal: 150.94 mm\n",
      "Step 6:\n",
      "  Current Position: [0.0826 0.078  0.1391]\n",
      "  Goal Position: [0.21703888 0.04528046 0.18339357]\n",
      "  Distance to Goal: 145.28 mm\n",
      "Step 7:\n",
      "  Current Position: [0.0858 0.075  0.1454]\n",
      "  Goal Position: [0.21703888 0.04528046 0.18339357]\n",
      "  Distance to Goal: 139.82 mm\n",
      "Step 8:\n",
      "  Current Position: [0.0895 0.0721 0.1504]\n",
      "  Goal Position: [0.21703888 0.04528046 0.18339357]\n",
      "  Distance to Goal: 134.44 mm\n",
      "Step 9:\n",
      "  Current Position: [0.0936 0.0693 0.1542]\n",
      "  Goal Position: [0.21703888 0.04528046 0.18339357]\n",
      "  Distance to Goal: 129.10 mm\n",
      "Step 10:\n",
      "  Current Position: [0.0978 0.0667 0.1573]\n",
      "  Goal Position: [0.21703888 0.04528046 0.18339357]\n",
      "  Distance to Goal: 123.93 mm\n",
      "Step 11:\n",
      "  Current Position: [0.1019 0.0642 0.1597]\n",
      "  Goal Position: [0.21703888 0.04528046 0.18339357]\n",
      "  Distance to Goal: 119.06 mm\n",
      "Step 12:\n",
      "  Current Position: [0.1061 0.0619 0.1617]\n",
      "  Goal Position: [0.21703888 0.04528046 0.18339357]\n",
      "  Distance to Goal: 114.26 mm\n",
      "Step 13:\n",
      "  Current Position: [0.1103 0.0597 0.1634]\n",
      "  Goal Position: [0.21703888 0.04528046 0.18339357]\n",
      "  Distance to Goal: 109.55 mm\n",
      "Step 14:\n",
      "  Current Position: [0.1144 0.0577 0.1651]\n",
      "  Goal Position: [0.21703888 0.04528046 0.18339357]\n",
      "  Distance to Goal: 104.99 mm\n",
      "Step 15:\n",
      "  Current Position: [0.1186 0.0557 0.1668]\n",
      "  Goal Position: [0.21703888 0.04528046 0.18339357]\n",
      "  Distance to Goal: 100.37 mm\n",
      "Step 16:\n",
      "  Current Position: [0.1228 0.0539 0.1685]\n",
      "  Goal Position: [0.21703888 0.04528046 0.18339357]\n",
      "  Distance to Goal: 95.80 mm\n",
      "Step 17:\n",
      "  Current Position: [0.1269 0.0522 0.1701]\n",
      "  Goal Position: [0.21703888 0.04528046 0.18339357]\n",
      "  Distance to Goal: 91.38 mm\n",
      "Step 18:\n",
      "  Current Position: [0.1311 0.0506 0.1717]\n",
      "  Goal Position: [0.21703888 0.04528046 0.18339357]\n",
      "  Distance to Goal: 86.89 mm\n",
      "Step 19:\n",
      "  Current Position: [0.1353 0.0491 0.1734]\n",
      "  Goal Position: [0.21703888 0.04528046 0.18339357]\n",
      "  Distance to Goal: 82.44 mm\n",
      "Step 20:\n",
      "  Current Position: [0.1394 0.0476 0.175 ]\n",
      "  Goal Position: [0.21703888 0.04528046 0.18339357]\n",
      "  Distance to Goal: 78.13 mm\n",
      "Step 21:\n",
      "  Current Position: [0.1436 0.0463 0.1766]\n",
      "  Goal Position: [0.21703888 0.04528046 0.18339357]\n",
      "  Distance to Goal: 73.76 mm\n",
      "Step 22:\n",
      "  Current Position: [0.1478 0.0451 0.1782]\n",
      "  Goal Position: [0.21703888 0.04528046 0.18339357]\n",
      "  Distance to Goal: 69.43 mm\n",
      "Step 23:\n",
      "  Current Position: [0.1519 0.0439 0.1798]\n",
      "  Goal Position: [0.21703888 0.04528046 0.18339357]\n",
      "  Distance to Goal: 65.25 mm\n",
      "Step 24:\n",
      "  Current Position: [0.1558 0.0428 0.1814]\n",
      "  Goal Position: [0.21703888 0.04528046 0.18339357]\n",
      "  Distance to Goal: 61.32 mm\n",
      "Step 25:\n",
      "  Current Position: [0.1594 0.0418 0.183 ]\n",
      "  Goal Position: [0.21703888 0.04528046 0.18339357]\n",
      "  Distance to Goal: 57.75 mm\n",
      "Step 26:\n",
      "  Current Position: [0.1629 0.0408 0.1845]\n",
      "  Goal Position: [0.21703888 0.04528046 0.18339357]\n",
      "  Distance to Goal: 54.34 mm\n",
      "Step 27:\n",
      "  Current Position: [0.1661 0.0399 0.1861]\n",
      "  Goal Position: [0.21703888 0.04528046 0.18339357]\n",
      "  Distance to Goal: 51.29 mm\n",
      "Step 28:\n",
      "  Current Position: [0.1692 0.0391 0.1876]\n",
      "  Goal Position: [0.21703888 0.04528046 0.18339357]\n",
      "  Distance to Goal: 48.42 mm\n",
      "Environment Terminated!\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO  # Replace PPO with your RL algorithm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Path to the trained model\n",
    "model_path = \"C:\\\\Users\\\\jarro\\\\Documents\\\\GitHub\\\\Reinforcement learning\\\\model.zip\"  # Replace with the actual path to your model\n",
    "\n",
    "# Load the trained model\n",
    "model = PPO.load(model_path)  # Replace PPO with the correct algorithm\n",
    "\n",
    "# Initialize the environment\n",
    "env = OT2Env(render=True)  # Initialize the environment with rendering disabled\n",
    "\n",
    "# Test the trained model\n",
    "def test_model(env, model, max_steps=1000):\n",
    "    \"\"\"\n",
    "    Test the trained model to move the pipette to the target location.\n",
    "\n",
    "    Args:\n",
    "        env: The testing environment.\n",
    "        model: The trained RL model.\n",
    "        max_steps: Maximum number of steps for the test.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Reset the environment\n",
    "    obs, _ = env.reset()\n",
    "\n",
    "    print(f\"Goal Position: {env.goal_position}\")\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        # Predict the action for the current state\n",
    "        action, _ = model.predict(obs, deterministic=True)  # Use deterministic=True for testing\n",
    "        obs, _, terminated, truncated, _ = env.step(action)  # Execute the action\n",
    "\n",
    "        # Calculate the accuracy (distance to the goal) in mm\n",
    "        current_position = obs[:3]  # Extract the current position from the observation\n",
    "        goal_position = env.goal_position\n",
    "        distance_to_goal = np.linalg.norm(goal_position - current_position) * 1000  # Convert to mm\n",
    "\n",
    "        # Print debug information\n",
    "        print(f\"Step {step + 1}:\")\n",
    "        print(f\"  Current Position: {current_position}\")\n",
    "        print(f\"  Goal Position: {goal_position}\")\n",
    "        print(f\"  Distance to Goal: {distance_to_goal:.2f} mm\")\n",
    "\n",
    "        # Check if the goal is reached or the environment signals termination\n",
    "        if terminated or distance_to_goal <= 1.0:  # 1 mm accuracy requirement\n",
    "            print(\"Goal Reached!\" if distance_to_goal <= 1.0 else \"Environment Terminated!\")\n",
    "            break\n",
    "\n",
    "    # Close the environment\n",
    "    env.close()\n",
    "\n",
    "\n",
    "# Run the test\n",
    "if __name__ == \"__main__\":\n",
    "    test_model(env, model, max_steps=1000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "block_2B_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
