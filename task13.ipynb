{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "from sim_class import Simulation\n",
    "\n",
    "class OT2Env(gym.Env):\n",
    "    def __init__(self, max_steps=1000):\n",
    "        super(OT2Env, self).__init__()\n",
    "        self.render = render\n",
    "        self.max_steps = max_steps\n",
    "\n",
    "        # Create the simulation environment\n",
    "        self.sim = Simulation(num_agents=1)\n",
    "\n",
    "        # Define action and observation space\n",
    "        # They must be gym.spaces objects\n",
    "        self.action_space = spaces.Box(low=np.array([-1, -1, -1, 0]), high=np.array([1, 1, 1, 1]), shape=(4,), dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(6,), dtype=np.float32)\n",
    "        # keep track of the number of steps\n",
    "        self.steps = 0\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        # being able to set a seed is required for reproducibility\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        # Reset the state of the environment to an initial state\n",
    "        # set a random goal position for the agent, consisting of x, y, and z coordinates within the working area (you determined these values in the previous datalab task)\n",
    "        # ['-0.18700', '0.21950', '0.16950']\n",
    "        # ['0.25300', '-0.17050', '0.16940']\n",
    "        # ['0.25300', '-0.17050', '0.28950']\n",
    "        # ['-0.18700', '0.21950', '0.28950']\n",
    "        # ['-0.18700', '-0.17050', '0.16950']\n",
    "        # ['0.25300', '0.21950', '0.16950']\n",
    "        # ['0.25300', '0.21950', '0.28950']\n",
    "        # ['-0.18700', '-0.17050', '0.28950']\n",
    "\n",
    "        self.goal_position = np.array([\n",
    "            np.random.uniform(-0.18700, 0.25300),\n",
    "            np.random.uniform(-0.17050, 0.21950),\n",
    "            np.random.uniform(0.16940, 0.28950)\n",
    "        ], dtype=np.float32)\n",
    "        # Call the environment reset function\n",
    "        observation = self.sim.reset(num_agents=1)\n",
    "        #print(observation)\n",
    "        # now we need to process the observation and extract the relevant information, the pipette position, convert it to a numpy array, and append the goal position and make sure the array is of type np.float32    \n",
    "        #{'robotId_2': {'joint_states': {'joint_0': {'position': 0.0, 'velocity': 0.0, 'reaction_forces': (0.0, 0.0, 0.0, 0.0, 0.0, 0.0), 'motor_torque': 0.0}, 'joint_1': {'position': 0.0, 'velocity': 0.0, 'reaction_forces': (0.0, 0.0, 0.0, 0.0, 0.0, 0.0), 'motor_torque': 0.0}, 'joint_2': {'position': 0.0, 'velocity': 0.0, 'reaction_forces': (0.0, 0.0, 0.0, 0.0, 0.0, 0.0), 'motor_torque': 0.0}}, 'robot_position': [0.0, 0.0, 0.03], 'pipette_position': [0.073, 0.0895, 0.1195]}}\n",
    "        pipette_position = np.array(observation[f'robotId_{self.sim.robotIds[0]}']['pipette_position'], dtype=np.float32)\n",
    "\n",
    "        observation = np.concatenate([pipette_position, self.goal_position], axis=0)\n",
    "\n",
    "        # Reset the number of steps\n",
    "        self.steps = 0\n",
    "\n",
    "        return observation, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        # Execute one time step within the environment\n",
    "        # since we are only controlling the pipette position, we accept 3 values for the action and need to append 0 for the drop action\n",
    "        action = np.append(action, 0)\n",
    "\n",
    "        # Call the environment step function\n",
    "        observation = self.sim.run([action]) # Why do we need to pass the action as a list? Think about the simulation class.\n",
    "        # now we need to process the observation and extract the relevant information, the pipette position, convert it to a numpy array, and append the goal position and make sure the array is of type np.float32\n",
    "        pipette_position = np.array(observation[f'robotId_{self.sim.robotIds[0]}']['pipette_position'], dtype=np.float32)\n",
    "\n",
    "\n",
    "\n",
    "        observation = np.concatenate([pipette_position, self.goal_position], axis=0)\n",
    "        # Calculate the reward, this is something that you will need to experiment with to get the best results\n",
    "        reward = float(-np.linalg.norm(pipette_position - self.goal_position)) # we can use the L2 norm to calculate the distance between the pipette position and the goal position\n",
    "        \n",
    "        # next we need to check if the if the task has been completed and if the episode should be terminated\n",
    "        # To do this we need to calculate the distance between the pipette position and the goal position and if it is below a certain threshold, we will consider the task complete. \n",
    "        # What is a reasonable threshold? Think about the size of the pipette tip and the size of the plants.\n",
    "        distance = np.linalg.norm(pipette_position - self.goal_position)\n",
    "        if distance < 0.05:\n",
    "            terminated = True\n",
    "            # we can also give the agent a positive reward for completing the task\n",
    "            reward = float(100)\n",
    "        else:\n",
    "            terminated = False\n",
    "\n",
    "        # next we need to check if the episode should be truncated, we can check if the current number of steps is greater than the maximum number of steps\n",
    "        if self.steps >= self.max_steps:\n",
    "            truncated = True\n",
    "        else:\n",
    "            truncated = False\n",
    "\n",
    "        info = {} # we don't need to return any additional information\n",
    "\n",
    "        # increment the number of steps\n",
    "        self.steps += 1\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def render(self):\n",
    "        pass\n",
    "    \n",
    "    def close(self):\n",
    "        self.sim.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goal Position: [0.13648537 0.20475353 0.19927526]\n",
      "Test Results:\n",
      "  Goal Position: [0.13648537 0.20475353 0.19927526]\n",
      "  Final Position: [0.0716 0.0891 0.1201]\n",
      "  Distance to Goal: 154.45 mm\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO  # Replace PPO with your RL algorithm\n",
    "import numpy as np\n",
    "\n",
    "# Path to the trained model\n",
    "model_path = \"C:\\\\Users\\\\jarro\\\\Documents\\\\GitHub\\\\Reinforcement learning\\\\model.zip\"  # Replace with the actual path to your model\n",
    "\n",
    "# Load the trained model\n",
    "model = PPO.load(model_path)  # Replace PPO with the correct algorithm\n",
    "\n",
    "# Initialize the environment\n",
    "env = OT2Env(render=True)  # Enable rendering for visualization\n",
    "# Test the trained model\n",
    "def test_model(env, model):\n",
    "    \"\"\"\n",
    "    Test the trained model to move the pipette directly to the target location.\n",
    "\n",
    "    Args:\n",
    "        env: The testing environment.\n",
    "        model: The trained RL model.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Reset the environment\n",
    "    obs, _ = env.reset()\n",
    "\n",
    "    print(f\"Goal Position: {env.goal_position}\")\n",
    "\n",
    "    # Predict the action for the current state\n",
    "    action, _ = model.predict(obs, deterministic=True)  # Use deterministic=True for testing\n",
    "    obs, _, _, _, _ = env.step(action)  # Execute the action\n",
    "\n",
    "    # Calculate the accuracy (distance to the goal) in mm\n",
    "    current_position = obs[:3]  # Extract the current position from the observation\n",
    "    goal_position = env.goal_position\n",
    "    distance_to_goal = np.linalg.norm(goal_position - current_position) * 1000  # Convert to mm\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Test Results:\")\n",
    "    print(f\"  Goal Position: {goal_position}\")\n",
    "    print(f\"  Final Position: {current_position}\")\n",
    "    print(f\"  Distance to Goal: {distance_to_goal:.2f} mm\")\n",
    "\n",
    "    # Close the environment\n",
    "    env.close()\n",
    "\n",
    "\n",
    "# Run the test\n",
    "if __name__ == \"__main__\":\n",
    "    test_model(env, model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "block_2B_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
